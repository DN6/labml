{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bgLz9Gojwcu-"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/labmlai/labml/blob/master/samples/wandb/cifar10.ipynb)\n",
    "\n",
    "## CIFAR-10 Sample\n",
    "\n",
    "This notebook trains a VGG model on CIFAR-10 dataset.\n",
    "\n",
    "[W&B dashboard for a sample run](https://wandb.ai/vpj/labml/runs/f2u6ip41?workspace=user-vpj)\n",
    "\n",
    "[labml.ai monitoring](https://app.labml.ai/run/451082b89e7f11ebbc450242ac1c0002)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8QoEceEJwv5Y"
   },
   "source": [
    "Install `labml` and `wandb` packages for monitoring and organizing experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mVYOkez1s5X_"
   },
   "outputs": [],
   "source": [
    "!pip install labml wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oVsf2UXxw5OI"
   },
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "bNCLoSMyroGJ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from labml import lab, tracker, experiment, monit, logger\n",
    "from labml.logger import Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AQ6D2It8xB6g"
   },
   "source": [
    "VGG model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "CPOGCcQ_syXL"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        in_channels = 3\n",
    "        for block in [[64, 64], [128, 128], [256, 256, 256], [512, 512, 512], [512, 512, 512]]:\n",
    "            for channels in block:\n",
    "                layers += [nn.Conv2d(in_channels, channels, kernel_size=3, padding=1),\n",
    "                           nn.BatchNorm2d(channels),\n",
    "                           nn.ReLU(inplace=True)]\n",
    "                in_channels = channels\n",
    "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        self.fc = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L-jYvGrXxIP5"
   },
   "source": [
    "A simple class to create the training and validation data loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "LQQpT6vpyD_g"
   },
   "outputs": [],
   "source": [
    "class DataLoaderFactory:\n",
    "    def __init__(self):\n",
    "        data_transform =  transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "        ])\n",
    "\n",
    "        self.dataset = [\n",
    "                        datasets.CIFAR10(str(lab.get_data_path()),\n",
    "                            train=False,\n",
    "                            download=True,\n",
    "                            transform=data_transform),\n",
    "                        datasets.CIFAR10(str(lab.get_data_path()),\n",
    "                            train=True,\n",
    "                            download=True,\n",
    "                            transform=data_transform),\n",
    "        ]\n",
    "     \n",
    "    def __call__(self, train, batch_size):\n",
    "        return torch.utils.data.DataLoader(self.dataset[train],\n",
    "                                           batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RuJ8bLudxNFV"
   },
   "source": [
    "Model training function for a single epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "a_4ACTp4tFvQ"
   },
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_loader, device):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in monit.enum(\"Train\", train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        tracker.add_global_step(data.shape[0])\n",
    "        tracker.save({'loss.train': loss})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yM2Y8u8uxUf_"
   },
   "source": [
    "Function to test the model on validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "dmX0C2m8tLbk"
   },
   "outputs": [],
   "source": [
    "def validate(model, valid_loader, device):\n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in monit.iterate(\"valid\", valid_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            output = model(data)\n",
    "            valid_loss += F.cross_entropy(output, target,\n",
    "                                          reduction='sum').item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    valid_loss /= len(valid_loader.dataset)\n",
    "    valid_accuracy = 100. * correct / len(valid_loader.dataset)\n",
    "\n",
    "    tracker.save({'loss.valid': valid_loss, 'accuracy.valid': valid_accuracy})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HJ3i5W1cxeCB"
   },
   "source": [
    "Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "xF_o7VFltSDw"
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    configs = {\n",
    "        'epochs': 50,\n",
    "        'learning_rate': 2.5e-4,\n",
    "        'device': \"cuda:0\" if torch.cuda.is_available() else \"cpu\",\n",
    "        'batch_size': 1024,\n",
    "    }\n",
    "\n",
    "    device = torch.device(configs['device'])\n",
    "    dl_factory = DataLoaderFactory()\n",
    "\n",
    "    train_loader = dl_factory(True, configs['batch_size'])\n",
    "    valid_loader = dl_factory(False, configs['batch_size'])\n",
    "\n",
    "    model = Net().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=configs['learning_rate'])\n",
    "\n",
    "    experiment.create(name='cifar10')\n",
    "    experiment.configs(configs)\n",
    "    experiment.add_pytorch_models(dict(model=model))\n",
    "\n",
    "    with experiment.start():\n",
    "        for _ in monit.loop(range(1, configs['epochs'] + 1)):\n",
    "            torch.cuda.empty_cache()\n",
    "            train(model, optimizer, train_loader, device)\n",
    "            validate(model, valid_loader, device)\n",
    "            logger.log()\n",
    "\n",
    "    experiment.save_checkpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "id": "8I5K5hDKtkTM",
    "outputId": "a6f449ec-1cc5-4045-ad02-f8a77ed1b2b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"overflow-x: scroll;\">\n",
       "\n",
       "<strong><span style=\"text-decoration: underline\">cifar10</span></strong>: <span style=\"color: #208FFB\">451082b89e7f11ebbc450242ac1c0002</span>\n",
       "\t[dirty]: <strong><span style=\"color: #DDB62B\">\"\"</span></strong>\n",
       "<span style=\"color: #C5C1B4\"></span>\n",
       "<span style=\"color: #C5C1B4\">--------------------------------------------------</span><span style=\"color: #DDB62B\"><strong><span style=\"text-decoration: underline\"></span></strong></span>\n",
       "<span style=\"color: #DDB62B\"><strong><span style=\"text-decoration: underline\">LABML WARNING</span></strong></span>\n",
       "<span style=\"color: #DDB62B\"><strong><span style=\"text-decoration: underline\"></span></strong></span>LabML App Warning: <span style=\"color: #60C6C8\">empty_token: </span><strong>Please create a valid token at https://app.labml.ai.</strong>\n",
       "<strong>Click on the experiment link to monitor the experiment and add it to your experiments list.</strong><span style=\"color: #C5C1B4\"></span>\n",
       "<span style=\"color: #C5C1B4\">--------------------------------------------------</span>\n",
       "<span style=\"color: #208FFB\">Monitor experiment at </span><a href='https://app.labml.ai/run/451082b89e7f11ebbc450242ac1c0002' target='blank'>https://app.labml.ai/run/451082b89e7f11ebbc450242ac1c0002</a>\n",
       "<strong><span style=\"color: #DDB62B\">  50,001:  </span></strong>Train:<span style=\"color: #C5C1B4\"> 100%</span><span style=\"color: #208FFB\"> 185,896ms  </span>valid:<span style=\"color: #C5C1B4\"> 100%</span><span style=\"color: #208FFB\"> 4,826ms  </span> loss.train: <span style=\"color: #C5C1B4\"> 1.11869</span> loss.valid: <strong> 1.27762</strong> accuracy.valid: <strong> 53.6400</strong>  <span style=\"color: #208FFB\">190,722ms</span><span style=\"color: #D160C4\">  0:03m/  2:35m  </span>\n",
       "<strong><span style=\"color: #DDB62B\"> 100,001:  </span></strong>Train:<span style=\"color: #C5C1B4\"> 100%</span><span style=\"color: #208FFB\"> 184,599ms  </span>valid:<span style=\"color: #C5C1B4\"> 100%</span><span style=\"color: #208FFB\"> 5,520ms  </span> loss.train: <span style=\"color: #C5C1B4\">0.844425</span> loss.valid: <strong> 0.95428</strong> accuracy.valid: <strong> 67.3100</strong>  <span style=\"color: #208FFB\">190,961ms</span><span style=\"color: #D160C4\">  0:06m/  2:32m  </span>\n",
       "<strong><span style=\"color: #DDB62B\"> 150,001:  </span></strong>Train:<span style=\"color: #C5C1B4\"> 100%</span><span style=\"color: #208FFB\"> 184,558ms  </span>valid:<span style=\"color: #C5C1B4\"> 100%</span><span style=\"color: #208FFB\"> 5,822ms  </span> loss.train: <span style=\"color: #C5C1B4\">0.659029</span> loss.valid: <strong> 0.98986</strong> accuracy.valid: <strong> 67.0800</strong>  <span style=\"color: #208FFB\">190,687ms</span><span style=\"color: #D160C4\">  0:09m/  2:29m  </span>\n",
       "<strong><span style=\"color: #DDB62B\"> 200,001:  </span></strong>Train:<span style=\"color: #C5C1B4\"> 100%</span><span style=\"color: #208FFB\"> 185,930ms  </span>valid:<span style=\"color: #C5C1B4\"> 100%</span><span style=\"color: #208FFB\"> 5,974ms  </span> loss.train: <span style=\"color: #C5C1B4\">0.464776</span> loss.valid: <strong> 0.93130</strong> accuracy.valid: <strong> 70.2600</strong>  <span style=\"color: #208FFB\">190,705ms</span><span style=\"color: #D160C4\">  0:12m/  2:26m  </span>\n",
       "<strong><span style=\"color: #DDB62B\"> 230,721:  </span></strong>Train:<span style=\"color: #C5C1B4\">  61%</span><span style=\"color: #208FFB\"> 185,866ms  </span>valid:<span style=\"color: #C5C1B4\"> 100%</span><span style=\"color: #208FFB\"> 5,974ms  </span> loss.train: <strong>0.307179</strong> loss.valid: <span style=\"color: #C5C1B4\"> 0.93130</span> accuracy.valid: <span style=\"color: #C5C1B4\"> 70.2600</span>  <span style=\"color: #208FFB\">190,988ms</span><span style=\"color: #D160C4\">  0:14m/  2:24m  </span></pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mvpj\u001B[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.26<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">zesty-universe-3</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/vpj/uncategorized\" target=\"_blank\">https://wandb.ai/vpj/uncategorized</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/vpj/uncategorized/runs/f2u6ip41\" target=\"_blank\">https://wandb.ai/vpj/uncategorized/runs/f2u6ip41</a><br/>\n",
       "                Run data is saved locally in <code>/content/wandb/run-20210416_064503-f2u6ip41</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bp6IdiO6tk3p"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CIFAR-10",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}